千辛万苦训练好模型, 上线遇到问题? Paddle预测库轻松搞定模型部署
===============

0. 深度学习inference概述
------------

如果问在深度学习实践中，最难的部分是什么？猜测80%的开发者都会说：“当然是调参啊”。为什么难呢？因为调参就像厨师根据食材找到了料理配方，药剂师根据药材找到了药方，充满了玄幻色彩。小编却以为，掌握了调参，顶多算深度学习的绝学掌握了一半。“另一半是啥？”另一半就是“部署预测模型”。这有什么难的？好，我们打个比方，前面那位大厨会很多料理配方对吧。两个小时内，来一套满汉全席。“……”虽然比喻有些夸张，不过这也道出了深度学习模型训练和推理预测的关系。我们知道，深度学习一般分为训练和推理两个部分, 训练是神经网络"学习"的过程, 主要关注如何搜索和求解模型参数, 发现训练数据中的规律. 有了训练好的模型之后, 就要在在线环境中使用模型, 这个过程叫做推理, 推理过程中主要关注计算的性能. 

当我们千辛万苦训练好模型，终于要上线了，这个时候会遇到各种问题：

百度飞桨提供了这样一款性能强劲, 上手简单的推理引擎, 经过多个版本的升级迭代, 飞桨推理引擎已经具备了良好的用户体验, 主要有以下优势:

* 通用性, 复用了飞桨训练前向代码, 支持所有飞桨训练产出模型.

* 多语言, 飞桨推理引擎完整支持C++, Python, C, Go和R的调用, 其中C++、Python API经过了多次迭代优化，稳定可靠，优先推荐使用.

* 多平台, 支持X86 CPU, NVIDIA Server, NVIDIA Jetson等硬件平台, 满足各种上线环境要求.

* 高性能, 针对不同平台深度优化, 提供最优的性能体验.


下面我们一起来走进飞桨推理世界吧!


1. 适配多个硬件平台
----------


2. 极致的性能优化
----------


3. 支持多种语言调用
----------


4. 使用示例
----------


1. 模型保存
------------

首先, 我们需要训练并保存一个模型, Paddle提供了一个内置的函数 `save_inference_model`, 将训练模型保存为预测模型.

.. code:: python
    
    from paddle import fluid

    place = fluid.CPUPlace()
    executor = fluid.Executor(place)

    image = fluid.data(name="image", shape=[None, 28, 28], dtype="float32")
    label = fluid.data(name="label", shape=[None, 1], dtype="int64")

    feeder = fluid.DataFeeder(feed_list=[image, label], place=place)
    predict = fluid.layers.fc(input=image, size=10, act='softmax')

    loss = fluid.layers.cross_entropy(input=predict, label=label)
    avg_loss = fluid.layers.mean(loss)

    executor.run(fluid.default_startup_program())

    # 保存预测模型到model目录中, 只保存与输入image和输出predict相关的部分网络
    fluid.io.save_inference_model("model", feed_var_names=["image"],
        target_vars=[predict]. executor=executor)


.. tip::

    `save_inference_model`根据预测需要的输入和输出, 对训练模型进行剪枝, 去除和预测无关部分, 得到的模型相比训练更加精简, 适合优化和部署.


2. 预测加载
-----------

有了预测模型之后, 就可以使用预测库了, Paddle提供了 AnalysisConfig 用于管理预测部署的各种设置, 用户可以根据自己的上线环境, 打开各种优化.

首先我们创建一个config

.. code:: python

    from paddle.fluid.core import AnalysisConfig

    # 创建配置对象
    config = AnalysisConfig("./model")



在Intel CPU上, 若硬件支持, 可以打开 `DNNL`_ (Deep Neural Network Library, 原名MKLDNN) 优化, 这是一个Intel开源的高性能计算库, 用于Intel架构的处理器和图形处理器上的深度学习优化, 飞桨推理引擎在后端将自动调用.

.. _DNNL: https://github.com/intel/mkl-dnn.git


.. code:: python

    config.enable_mkldnn()



对于需要使用Nvidia GPU用户, 只需要一行配置, 飞桨就会自动将计算切换到GPU上

.. code:: python

    # 在 GPU 0 上初始化 100 MB 显存。这只是一个初始值，实际显存可能会动态变化。
    config.enable_use_gpu(100, 0)


飞桨推理引擎提供了zero copy的方式管理输入和输出, 减少拷贝

.. code:: python

    # 打开zero copy
    config.switch_use_feed_fetch_ops(False)
    config.switch_specify_input_names(True)


设置好预测的配置后，就可以创建预测器了。


.. code:: python

    from paddle.fluid.core import create_paddle_predictor

    predictor = create_paddle_predictor(config)


.. tip::

    Paddle 预测提供了多项图优化，创建预测器时将会加载预测模型并自动进行图优化，以增强预测性能。


3. 运行
------------

创建好predictor之后, 只需要传入数据就可以运行预测了, 这里假设我们已经将输入数据读入了一个numpy.ndarray数组中.


Paddle 提供了简单易用的API来管理输入和输出. 首先将输入数据传入predictor


.. code:: python

    input_names = predictor.get_input_names()
    # 得到输入 ZeroCopyTensor，前面保存的模型只有一个输入图片，多输入下的操作是类似的。
    input_tensor = predictor.get_input_tensor(input_names[0])

    input_tensor.copy_from_cpu(input_data.reshape([1, 28, 28]).astype("float32"))


运行推理引擎, 这里将会执行真正的计算


.. code:: python

    predictor.zero_copy_run()


解析结果到一个numpy数组中


.. code:: python

    ouput_names = predictor.get_output_names()
    # 获取输出 ZeroCopyTensor
    output_tensor = predictor.get_output_tensor(output_names[0])

    # 得到一个 numpy.ndarray 封装的输出数据
    output_data = output_tensor.copy_to_cpu()



4. 进阶
-------------

4.1 使用 TensorRT 加速预测
~~~~~~~~~~~~

Paddle 预测集成了 TensorRT 引擎。使用 GPU 预测时，开启 TensorRT 在一些模型上可以提高性能。


.. code:: python

    config.enable_tensorrt_engine(precision_mode=AnalysisConfig.Precision.Float32,
                                  use_calib_mode=True)


4.2 使用Paddle-Lite优化
~~~~~~~~

Paddle 预测集成了 Paddle-Lite 预测加速引擎。开启 Paddle-Lite 在一些模型上可以增强性能。

.. code:: python

    config.enable_lite_engine(precision_mode=AnalysisConfig.Precision.Float32)


4.3 在其它语言中使用 Paddle 预测
~~~~~~~

        
